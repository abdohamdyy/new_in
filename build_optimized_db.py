#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ChromaDB ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ø³Ø±Ø¹Ø© - Ø¥ØµØ¯Ø§Ø± TURBO
Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ø§Ø³ØªØºÙ„Ø§Ù„ 12 ÙƒÙˆØ±
- Ø¯ÙØ¹Ø§Øª Ø¹Ù…Ù„Ø§Ù‚Ø© (Mega Batches)
- ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Memory Optimization)
- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª HNSW ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
- Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…Ù†Ø·Ù‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠ
"""

import json
import os
import gc
import logging
import torch
import psutil
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from config import Config
import chromadb
from chromadb.config import Settings

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ logging Ø§Ù„Ù…Ø­Ø³Ù†
LOGS_DIR = 'logs'
# os.makedirs(LOGS_DIR, exist_ok=True) # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù„ÙˆØºØ§Øª - ğŸ ØªÙ… Ø§Ù„ØªØ¹Ø·ÙŠÙ„ Ù„Ø£Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ÙˆØ±Ø¨Ø·Ù‡ Ø¹Ø¨Ø± Docker Compose

logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, 'db_build.log'), encoding='utf-8'), # ğŸ¯ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù„ÙˆØºØ§Øª
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class TurboDatabaseBuilder:
    """ğŸš€ Ø¨Ø§Ù†ÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© Ù„Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ø§Ù„Ù‚ÙˆÙŠØ©"""
    
    def __init__(self, data_file=None, db_directory=None):
        self.data_file = data_file or Config.DATA_FILE
        self.db_directory = db_directory or Config.DB_DIRECTORY
        self.batch_size = Config.CHROMA_SETTINGS.get('batch_size', 25000)
        self.max_workers = Config.PERFORMANCE_SETTINGS.get('max_workers', 11)
        
        # ğŸ¯ ØªØ¹Ø¯ÙŠÙ„: ØªÙ‡ÙŠØ¦Ø© Ø§Ø³Ù… Ø§Ù„ÙƒÙˆÙ„ÙƒØ´Ù† Ù…Ù† Ø§Ù„Ù€ config
        self.collection_name = Config.COLLECTION_NAME
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ±ÙØ±
        self.cpu_count = psutil.cpu_count(logical=True)
        self.memory_gb = psutil.virtual_memory().total / (1024**3)
        
        logger.info("ğŸ”¥ TURBO MODE ACTIVATED - Server Specs:")
        logger.info(f"   ğŸ’» CPU Cores: {self.cpu_count}")
        logger.info(f"   ğŸ§  RAM: {self.memory_gb:.1f}GB")
        logger.info(f"   âš¡ Max Workers: {self.max_workers}")
        logger.info(f"   ğŸ“¦ Batch Size: {self.batch_size:,}")
        
        os.makedirs(self.db_directory, exist_ok=True)
        
        self.chroma_client = self._create_turbo_chroma_client()
        
        self.hnsw_settings = {k: v for k, v in Config.CHROMA_SETTINGS.items() if k.startswith('hnsw_')}
        
        # ğŸ¯ ØªØ¹Ø¯ÙŠÙ„: ØªÙ‡ÙŠØ¦Ø© Ø§Ø³Ù… Ø§Ù„ÙƒÙˆÙ„ÙƒØ´Ù† Ù‡Ù†Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ ÙƒÙ„ Ù…ÙƒØ§Ù†
        self.collection_name = "medical_medications_v2_resumable"
        
        self.embedding_model = self._create_turbo_embedding_model()
        
        logger.info(f"ğŸš€ TURBO Builder initialized with {self.max_workers} workers!")

    def _create_turbo_chroma_client(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ChromaDB client Ù…Ø­Ø³Ù† Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰"""
        client = chromadb.PersistentClient(
            path=self.db_directory,
            settings=Settings(
                persist_directory=self.db_directory,
                anonymized_telemetry=False,
                allow_reset=True,
                is_persistent=True,
            )
        )
        logger.info(f"ğŸ”¥ ChromaDB TURBO client created: {self.db_directory}")
        return client

    def _get_existing_ids_turbo(self, collection):
        """âš¡ï¸ Ø¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ù€ IDs Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„ÙƒÙˆÙ„ÙƒØ´Ù† Ø¨Ø³Ø±Ø¹Ø©"""
        try:
            # ChromaDB Ù‚Ø¯ ØªÙØ±Ø¬Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ ØµÙØ­Ø§ØªØŒ Ù†Ø­ØªØ§Ø¬ Ù„Ø¬Ù„Ø¨Ù‡Ø§ ÙƒÙ„Ù‡Ø§
            all_ids = []
            # ChromaDB Ù„Ø§ ØªØ¯Ø¹Ù… Ø¬Ù„Ø¨ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø¯ÙˆÙ† Ø­Ø¯ Ø£Ù‚ØµÙ‰ØŒ Ù„Ø°Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ø±Ù‚Ù… ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹
            # ÙˆÙ†ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø£Ù‚Ù„ Ù…Ù†Ù‡ Ù„Ù†ØªÙˆÙ‚Ù.
            page_size = 50000  # Ø­Ø¬Ù… ØµÙØ­Ø© ÙƒØ¨ÙŠØ±
            last_count = page_size
            offset = 0
            
            logger.info(f"ğŸ” Fetching existing document IDs from '{self.collection_name}'...")
            
            # Ù†Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„Ø¬Ù„Ø¨ Ø·Ø§Ù„Ù…Ø§ Ø£Ù† Ø¢Ø®Ø± Ø¯ÙØ¹Ø© ÙƒØ§Ù†Øª Ù…Ù…ØªÙ„Ø¦Ø©
            while last_count == page_size:
                results = collection.get(limit=page_size, offset=offset, include=[])
                ids = results.get('ids', [])
                if not ids:
                    break
                all_ids.extend(ids)
                last_count = len(ids)
                offset += last_count

            logger.info(f"âœ… Found {len(all_ids):,} existing document IDs.")
            return set(all_ids)
        except Exception as e:
            logger.error(f"ğŸ’¥ Could not fetch existing IDs: {e}", exc_info=True)
            return set()

    def _create_turbo_embedding_model(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ¶Ù…ÙŠÙ† Ù…Ø­Ø³Ù† Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰"""
        torch.set_num_threads(self.max_workers)
        torch.set_num_interop_threads(self.max_workers)
        
        model = HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_SETTINGS['model_name'],
            model_kwargs=Config.EMBEDDING_SETTINGS['model_kwargs'],
            encode_kwargs=Config.EMBEDDING_SETTINGS['encode_kwargs']
        )
        
        logger.info(f"ğŸš€ TURBO Embedding Model loaded with {Config.EMBEDDING_SETTINGS['encode_kwargs']['batch_size']} batch size")
        return model

    def load_data_turbo(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø³Ø±Ø¹Ø© ÙØ§Ø¦Ù‚Ø©"""
        logger.info(f"ğŸš€ TURBO Loading data from: {self.data_file}")
        start_time = time.time()
        try:
            with open(self.data_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            load_time = time.time() - start_time
            logger.info(f"âš¡ LOADED {len(data):,} records in {load_time:.2f}s ({len(data)/load_time:.0f} records/sec)")
            return data
        except Exception as e:
            logger.error(f"ğŸ’¥ Error loading data: {str(e)}")
            raise
    
    def prepare_document_parallel(self, meds_batch):
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†ÙØ³ Ù…Ù†Ø·Ù‚ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯"""
        
        def process_single_med(med):
            """Ù†ÙØ³ Ù…Ù†Ø·Ù‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ø£ØµÙ„ÙŠ"""
            try:
                med_id = med.get("Ø§Ù„ÙƒÙˆØ¯")
                if not med_id:
                    # ğŸ¯ Ø®Ø·ÙˆØ© Ø­Ø§Ø³Ù…Ø©: Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ IDØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©
                    # Ù‡Ø°Ø§ ÙŠØ¶Ù…Ù† Ø£Ù† ÙƒÙ„ Ø³Ø¬Ù„ ÙŠÙ…ÙƒÙ† ØªØªØ¨Ø¹Ù‡ ÙˆØ§Ø³ØªØ¦Ù†Ø§ÙÙ‡
                    logger.warning(f"âš ï¸ Skipping record due to missing 'Ø§Ù„ÙƒÙˆØ¯' (ID). Record data: {str(med)[:100]}...")
                    return None

                # (Ù‡Ù†Ø§ Ù†ÙØ³ ÙƒÙˆØ¯ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ù…Ù† Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ø£ØµÙ„ÙŠ)
                name = str(med.get("Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠ", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"))
                scientific_name = str(med.get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ", ""))
                commercial_name = str(med.get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ", ""))
                active_ingredients = str(med.get("Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ø§Ù„Ø©", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"))
                diseases = str(med.get("Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ØªÙŠ ÙŠØ¹Ø§Ù„Ø¬Ù‡Ø§", "") or "")
                side_effects = str(med.get("Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©", "") or "")
                disease_symptoms = str(med.get("Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ø±Ø¶", "") or "")
                usage = str(med.get("Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…", ""))
                classification = str(med.get("Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"))
                pharma_form = str(med.get("Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ", ""))
                manufacturer = str(med.get("Ø§Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ù…ØµÙ†Ø¹Ø©", ""))
                under_age = str(med.get("Ù„Ø§ ÙŠØ³ØªØ®Ø¯Ù… ØªØ­Øª Ø¹Ù…Ø±", ""))
                pregnancy_safe = str(med.get("Ù‡Ù„ Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø¢Ù…Ù† Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­Ù…Ù„ØŸ", ""))
                breastfeeding_safe = str(med.get("Ù‡Ù„ ÙŠØªØ¹Ø§Ø±Ø¶ Ù…Ø¹ Ø§Ù„Ø±Ø¶Ø§Ø¹Ø©ØŸ", ""))
                prescription_required = str(med.get("Ù‡Ù„ ÙŠØªØ·Ù„Ø¨ ÙˆØµÙØ© Ø·Ø¨ÙŠØ©ØŸ", ""))
                dosage = str(med.get("Ø§Ù„Ø¬Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹ØªØ§Ø¯Ø©", ""))
                concentrations = str(med.get("Ø§Ù„ØªØ±Ø§ÙƒÙŠØ²", ""))
                
                diseases_cleaned = [d.strip() for d in diseases.split(",") if d.strip()]
                side_effects_cleaned = [s.strip() for s in side_effects.split(",") if s.strip()]
                disease_symptoms_cleaned = [s.strip() for s in disease_symptoms.split(",") if s.strip()]
                
                active_ingredients = active_ingredients.replace("\n", " ").strip()
                diseases = diseases.replace("\n", " ").strip()
                disease_symptoms = disease_symptoms.replace("\n", " ").strip()
                
                content_parts = []
                if diseases_cleaned:
                    content_parts.extend([
                        f"Ø§Ù„Ø£Ù…Ø±Ø§Ø¶: {diseases}",
                        f"ÙŠØ¹Ø§Ù„Ø¬: {' - '.join(diseases_cleaned)}",
                        f"ÙØ¹Ø§Ù„ Ù„Ø¹Ù„Ø§Ø¬: {' - '.join(diseases_cleaned)}"
                    ])
                
                if disease_symptoms_cleaned:
                    content_parts.extend([
                        f"Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶: {disease_symptoms}",
                        f"ÙŠØ®ÙÙ Ø£Ø¹Ø±Ø§Ø¶: {' - '.join(disease_symptoms_cleaned)}",
                        f"Ù…ÙÙŠØ¯ Ù„Ù„Ø£Ø¹Ø±Ø§Ø¶: {' - '.join(disease_symptoms_cleaned)}"
                    ])
                
                content_parts.append(f"Ø§Ù„Ø¯ÙˆØ§Ø¡: {name}")
                
                if active_ingredients.strip() and active_ingredients != "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ":
                    content_parts.append(f"Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø©: {active_ingredients}")
                
                page_content = "\n".join(content_parts)
                
                doc = Document(
                    page_content=page_content,
                    metadata={
                        "id": med_id, # ğŸ¯ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ø§Ù„Ù€ ID Ù‡Ù†Ø§
                        "search_priority": self._calculate_search_priority(diseases_cleaned, disease_symptoms_cleaned),
                        "indexed_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "under_age_numeric": self._extract_age_number(under_age),
                        "pregnancy_safe": "Ù†Ø¹Ù…" if pregnancy_safe.lower() in ['Ù†Ø¹Ù…', 'yes', 'safe', 'Ø¢Ù…Ù†'] else "Ù„Ø§",
                        "breastfeeding_safe": "Ù†Ø¹Ù…" if breastfeeding_safe.lower() in ['Ù†Ø¹Ù…', 'yes', 'safe', 'Ø¢Ù…Ù†'] else "Ù„Ø§",
                        "is_otc": "Ù†Ø¹Ù…" if med.get("OTC", "Ù„Ø§").lower() in ['Ù†Ø¹Ù…', 'yes'] else "Ù„Ø§",
                        "is_narcotic": "Ù†Ø¹Ù…" if med.get("Ù‡Ù„ Ù‡Ùˆ Ù…Ø®Ø¯Ø±ØŸ", "Ù„Ø§").lower() in ['Ù†Ø¹Ù…', 'yes'] else "Ù„Ø§",
                        "requires_prescription": "Ù†Ø¹Ù…" if prescription_required.lower() in ['Ù†Ø¹Ù…', 'yes'] else "Ù„Ø§",
                        "name": name,
                        "scientific_name": scientific_name,
                        "commercial_name": commercial_name,
                        "active_ingredients": active_ingredients,
                        "diseases": diseases,
                        "disease_symptoms": disease_symptoms,
                        "classification": classification,
                        "pharma_form": pharma_form,
                        "manufacturer": manufacturer,
                        "dosage": dosage,
                        "usage": usage,
                        "under_age": under_age,
                        "concentrations": concentrations,
                        "side_effects": side_effects,
                        "storage_temperature": med.get("Ø¯Ø±Ø¬Ø© Ø­Ø±Ø§Ø±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†", ""),
                        "unit": med.get("Ø§Ù„ÙˆØ­Ø¯Ø©", ""),
                        "section": med.get("Ø§Ù„Ù‚Ø³Ù…", ""),
                        "additional_materials": med.get("Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø¶Ø§ÙØ©", ""),
                        "contraindicated_materials": med.get("Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø­Ø¸ÙˆØ± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù…Ø¹Ù‡", ""),
                        "min_dosage": med.get("Ø£Ù‚Ù„ Ø¬Ø±Ø¹Ø©", ""),
                        "max_dosage": med.get("Ø£Ù‚ØµÙ‰ Ø¬Ø±Ø¹Ø©", ""),
                        "usage_after_opening": med.get("Ù…Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ù„ÙØªØ­", ""),
                        "seasonal": med.get("Ù‡Ù„ Ù‡Ùˆ Ø¯ÙˆØ§Ø¡ Ù…ÙˆØ³Ù…ÙŠØŸ", "Ù„Ø§"),
                        "requires_refrigeration": med.get("ÙŠØ­ÙØ¸ ÙÙŠ Ø§Ù„Ø«Ù„Ø§Ø¬Ø©", "Ù„Ø§"),
                        "has_expiry_date": med.get("Ù„Ù‡ ØªØ§Ø±ÙŠØ® ØµÙ„Ø§Ø­ÙŠØ©", ""),
                        "disease_symptoms_count": len(disease_symptoms_cleaned),
                        "diseases_count": len(diseases_cleaned),
                        "side_effects_count": len(side_effects_cleaned),
                        "has_dosage": bool(dosage.strip()),
                        "has_scientific_name": bool(scientific_name.strip()),
                        "has_commercial_name": bool(commercial_name.strip()),
                        "has_side_effects": bool(side_effects.strip()),
                    }
                )
                return doc
            except Exception as e:
                logger.warning(f"âš ï¸  Skipping corrupted record: {str(e)}")
                return None
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_single_med, med) for med in meds_batch]
            documents = [future.result() for future in as_completed(futures) if future.result() is not None]
        return documents
    
    def _extract_age_number(self, age_text):
        if not age_text or age_text.strip() == "": return 0
        import re
        numbers = re.findall(r'\d+', str(age_text))
        return int(numbers[0]) if numbers else 0
    
    def _calculate_search_priority(self, diseases_list, symptoms_list):
        count = len(diseases_list) + len(symptoms_list)
        if count >= 5: return "high"
        if count >= 2: return "medium"
        return "low"

    def build_database_turbo(self, raw_data):
        """
        ğŸ—ï¸ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù ÙˆÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©
        Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:
        1.  **Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù (Resumable):** ÙŠÙ‚Ø±Ø£ Ø§Ù„Ù€ IDs Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆÙŠØªØ®Ø·Ø§Ù‡Ø§.
        2.  **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ù„Ø¯ÙØ¹Ø§Øª (Batch Processing):** ÙŠØ¶ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© ÙŠÙ…ÙƒÙ† ØªØªØ¨Ø¹Ù‡Ø§.
        3.  **Ù…Ø¹Ø±ÙØ§Øª ØµØ±ÙŠØ­Ø© (Explicit IDs):** ÙŠØ³ØªØ®Ø¯Ù… "Ø§Ù„ÙƒÙˆØ¯" ÙƒÙ…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±.
        """
        
        # ğŸ¯ ÙØ­Øµ Ø¨Ù†ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if isinstance(raw_data, dict) and 'medicines' in raw_data:
            logger.info("âœ… Modern data structure detected, using 'medicines' list.")
            data = raw_data['medicines']
        else:
            logger.info("â„¹ï¸ Legacy data structure (direct list) detected.")
            data = raw_data

        logger.info("ğŸš€ RESUMABLE TURBO DATABASE BUILD STARTING...")
        
        # --- ğŸ¯ Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆÙ„ÙƒØ´Ù† Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¤Ù‡ ---
        logger.info(f"ğŸ”¥ Ensuring collection '{self.collection_name}' exists with optimized HNSW settings...")
        collection_metadata = {
            "description": "RESUMABLE TURBO Medical Database",
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "optimization_level": "TURBO_RESUMABLE_V2"
        }
        collection_metadata.update(self.hnsw_settings)
        
        collection = self.chroma_client.get_or_create_collection(
            name=self.collection_name,
            metadata=collection_metadata
        )
        
        # --- ğŸ¯ Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¬Ù„Ø¨ Ø§Ù„Ù€ IDs Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„ØªØ®Ø·ÙŠÙ‡Ø§ ---
        existing_ids = self._get_existing_ids_turbo(collection)
        
        # --- ğŸ¯ Ø§Ù„Ø®Ø·ÙˆØ© 3: ÙÙ„ØªØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø· ---
        logger.info("ğŸ” Filtering data to process only new records...")
        
        # ğŸš¨ ØªØ¹Ø¯ÙŠÙ„ Ø­Ø§Ø³Ù…: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† "Ø§Ù„ÙƒÙˆØ¯" Ù…ÙˆØ¬ÙˆØ¯ (Ù„ÙŠØ³ None) ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù†Øµ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        new_data = [
            med for med in data 
            if med.get("Ø§Ù„ÙƒÙˆØ¯") is not None and str(med.get("Ø§Ù„ÙƒÙˆØ¯")) not in existing_ids
        ]
        
        total_new_records = len(new_data)
        if total_new_records == 0:
            logger.info("âœ…ğŸ‰ Database is already up-to-date! No new records to process.")
            logger.info(f"ğŸ“Š Total records in DB: {len(existing_ids):,}")
            # Ù†Ø¹ÙŠØ¯ vectorstore Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ© Ø¹Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
            return Chroma(
                client=self.chroma_client,
                collection_name=self.collection_name,
                embedding_function=self.embedding_model
            )

        logger.info(f"âš¡ Found {total_new_records:,} new records to process out of {len(data):,} total.")
        
        # --- ğŸ¯ Ø§Ù„Ø®Ø·ÙˆØ© 4: Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª ---
        #  ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ: Ù†Ø³ØªØ®Ø¯Ù… Ø­Ø¬Ù… Ø¯ÙØ¹Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© ØµØºÙŠØ± ÙˆØ¢Ù…Ù† Ù„ØªØ¬Ù†Ø¨ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ­Ø¯ÙˆØ¯ ChromaDB
        processing_batch_size = 500 # ğŸš¨ Ø­Ø¬Ù… Ø¯ÙØ¹Ø© Ø¢Ù…Ù†
        
        total_batches = (total_new_records + processing_batch_size - 1) // processing_batch_size
        logger.info(f"ğŸ“¦ Processing {total_new_records:,} new records in {total_batches} manageable batches of up to {processing_batch_size:,} each.")
        
        vectorstore = Chroma(
            client=self.chroma_client,
            collection_name=self.collection_name,
            embedding_function=self.embedding_model
        )
        
        total_processed = 0
        overall_start = time.time()
        
        for batch_idx in range(total_batches):
            batch_start_time = time.time()
            start_idx = batch_idx * processing_batch_size
            end_idx = min((batch_idx + 1) * processing_batch_size, total_new_records)
            batch = new_data[start_idx:end_idx]
            
            logger.info(f"ğŸ”¥ TURBO Batch {batch_idx+1}/{total_batches}: Processing {len(batch):,} records")
            
            prep_start = time.time()
            documents = self.prepare_document_parallel(batch)
            # ğŸš¨ ØªØ¹Ø¯ÙŠÙ„ Ø­Ø§Ø³Ù…: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­ÙˆÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù€ IDs Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù‚Ø¨Ù„ Ø¥Ø±Ø³Ø§Ù„Ù‡Ø§
            ids = [str(doc.metadata['id']) for doc in documents]
            prep_time = time.time() - prep_start
            
            if not documents:
                logger.warning(f"âš ï¸ No processable documents in batch {batch_idx+1}. Skipping.")
                continue

            logger.info(f"âš¡ Prepared {len(documents):,} docs in {prep_time:.2f}s ({len(documents)/prep_time:.0f} docs/sec)")
            
            if documents:
                embed_start = time.time()
                # ğŸ¯ ØªØ¹Ø¯ÙŠÙ„ Ø­Ø§Ø³Ù…: Ø§Ø³ØªØ®Ø¯Ø§Ù… add_documents Ù…Ø¹ ids
                vectorstore.add_documents(documents=documents, ids=ids)
                embed_time = time.time() - embed_start
                
                total_processed += len(documents)
                batch_time = time.time() - batch_start_time
                
                docs_per_sec = len(documents) / batch_time if batch_time > 0 else 0
                embeddings_per_sec = len(documents) / embed_time if embed_time > 0 else 0
                progress = (total_processed / total_new_records) * 100 if total_new_records > 0 else 100
                
                logger.info(f"âœ… Batch {batch_idx+1} COMPLETE:")
                logger.info(f"   â±ï¸  Total time: {batch_time:.2f}s")
                logger.info(f"   ğŸ“Š Docs/sec: {docs_per_sec:.0f}")
                logger.info(f"   ğŸš€ Embeddings/sec: {embeddings_per_sec:.0f}")
                logger.info(f"   ğŸ“ˆ Progress: {progress:.1f}%")
                logger.info(f"   ğŸ’¾ Processed: {total_processed:,}/{total_new_records:,}")
            
            if (batch_idx + 1) % 3 == 0:
                logger.info("ğŸ§¹ Memory cleanup...")
                gc.collect()
                memory_info = psutil.virtual_memory()
                logger.info(f"ğŸ’¾ Memory usage: {memory_info.percent}% ({memory_info.used/1024**3:.1f}GB)")
        
        total_time = time.time() - overall_start
        logger.info("ğŸ‰" + "="*80)
        logger.info("ğŸš€ TURBO DATABASE BUILD/UPDATE COMPLETE!")
        logger.info(f"â±ï¸  Total time for this run: {total_time:.2f} seconds")
        logger.info(f"ğŸ“Š Total processed in this run: {total_processed:,} new documents")
        if total_time > 0:
            logger.info(f"ğŸš€ Overall speed for this run: {total_processed/total_time:.0f} docs/sec")
        
        try:
            final_count = collection.count()
            logger.info(f"âœ… Total documents in collection '{self.collection_name}': {final_count:,}")
        except Exception as e:
            logger.error(f"ğŸ’¥ Could not get final collection count: {e}")

        logger.info("ğŸ‰" + "="*80)
        
        return vectorstore
    
    def test_turbo_performance(self, vectorstore):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰"""
        logger.info("ğŸ”¥ TURBO PERFORMANCE TEST STARTING...")
        
        turbo_queries = [
            "ØµØ¯Ø§Ø¹", "Ø­Ù…Ù‰", "ÙƒØ­Ø©", "Ø£Ù„Ù… Ø§Ù„Ø¨Ø·Ù†", "Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ø­Ù„Ù‚",
            "Ø¨Ø±Ø¯", "Ø§Ù†ÙÙ„ÙˆÙ†Ø²Ø§", "Ø­Ø³Ø§Ø³ÙŠØ©", "Ø¥Ø³Ù‡Ø§Ù„", "Ù‚ÙŠØ¡",
            "Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„", "Ø£ÙŠØ¨ÙˆØ¨Ø±ÙˆÙÙŠÙ†", "Ø£Ø³Ø¨Ø±ÙŠÙ†", "Ù…Ø¶Ø§Ø¯ Ø­ÙŠÙˆÙŠ",
        ]
        
        search_times = []
        logger.info(f"ğŸ¯ Testing {len(turbo_queries)} queries...")
        
        for query in turbo_queries:
            start_time = time.time()
            vectorstore.similarity_search(query, k=10)
            query_time = time.time() - start_time
            search_times.append(query_time)
            
        avg_time = sum(search_times) / len(search_times)
        performance_grade = "ğŸ”¥ BLAZING FAST" if avg_time < 0.05 else "ğŸš€ VERY FAST" if avg_time < 0.1 else "âœ… FAST"
        
        logger.info("ğŸ“Š" + "="*50)
        logger.info("ğŸš€ TURBO PERFORMANCE RESULTS:")
        logger.info(f"âš¡ Average search time: {avg_time:.3f}s")
        logger.info(f"ğŸ¯ Performance grade: {performance_grade}")
        logger.info(f"ğŸ” Searches per second: {1/avg_time:.0f}")
        logger.info("ğŸ“Š" + "="*50)
        return {'performance_grade': performance_grade}

    def get_turbo_stats(self):
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
        if not os.path.exists(self.db_directory): return None
        
        total_size = sum(os.path.getsize(os.path.join(dirpath, f)) for dirpath, _, filenames in os.walk(self.db_directory) for f in filenames)
        size_mb = total_size / (1024 * 1024)
        
        stats = {'total_size_mb': round(size_mb, 2), 'directory': self.db_directory}
        try:
            collection = self.chroma_client.get_collection(self.collection_name)
            stats['collection_count'] = collection.count()
        except Exception as e:
            logger.warning(f"âš ï¸  Cannot get collection stats: {e}")
        return stats
    
    def build(self):
        """Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©"""
        try:
            overall_start = time.time()
            data = self.load_data_turbo()
            vectorstore = self.build_database_turbo(data)
            performance_results = self.test_turbo_performance(vectorstore)
            stats = self.get_turbo_stats()
            overall_time = time.time() - overall_start
            
            logger.info("ğŸ‰" + "="*80)
            logger.info("ğŸ”¥ğŸ”¥ğŸ”¥ TURBO BUILD SUMMARY ğŸ”¥ğŸ”¥ğŸ”¥")
            logger.info(f"â±ï¸  Total build time: {overall_time:.2f} seconds")
            if data:
                logger.info(f"ğŸ“Š Total records in source file: {len(data.get('medicines', data)):,}")
            if stats:
                logger.info(f"ğŸ’¾ Database size: {stats.get('total_size_mb', 'N/A'):.1f} MB")
                logger.info(f"ğŸ“ˆ Documents in collection '{self.collection_name}': {stats.get('collection_count', 'N/A'):,}")
            logger.info(f"âš¡ Search performance: {performance_results.get('performance_grade', 'N/A')}")
            logger.info("âœ… RESUMABLE TURBO OPTIMIZATIONS APPLIED")
            logger.info(f"   ğŸ”¥ {self.max_workers}-core parallel processing")
            logger.info(f"   ğŸ“¦ {self.batch_size:,} record mega-batches")
            logger.info("ğŸ‰" + "="*80)
            
            return vectorstore
        except Exception as e:
            logger.error(f"ğŸ’¥ TURBO BUILD FAILED: {str(e)}", exc_info=True)
            raise

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ø³Ø±Ø¹Ø©"""
    logger.info("ğŸš€ğŸš€ğŸš€ DUAYA TURBO DATABASE BUILDER ğŸš€ğŸš€ğŸš€")
    logger.info(f"ğŸ’ª Optimized for {psutil.cpu_count(logical=True)}-core CPU + {psutil.virtual_memory().total / (1024**3):.1f}GB RAM server")
    
    builder = TurboDatabaseBuilder()
    builder.build()
    
    logger.info("ğŸ‰ğŸ‰ğŸ‰ TURBO BUILD SUCCESS! ğŸ‰ğŸ‰ğŸ‰")
    logger.info("ğŸ”¥ Database is ready for BLAZING FAST searches!")

if __name__ == "__main__":
    main() 