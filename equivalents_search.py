# -*- coding: utf-8 -*-
"""
equivalents_search.py

ğŸ” Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø« (Querying Order):
1) Ù†Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ù‘Ø§Ù„Ø© Ø£ÙˆÙ„Ù‹Ø§ (active_ingredients) + scientific hint
2) Ø«Ù… Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ ÙÙ‚Ø·
3) Ø«Ù… Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ/Ø§Ù„Ø§Ø³Ù… (brand/commercial)
(Ø§Ù„ØªØ±ÙƒÙŠØ² Ù„Ø§ ÙŠÙØ³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹.. Ù‡Ù†Ø³ØªØ®Ø¯Ù…Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§ ÙÙŠ Ø§Ù„ÙØ±Ø²)

ğŸ“Š Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ÙØ±Ø² (Ranking in response):
- Ø£ÙˆÙ„Ø§Ù‹: Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¹Ù„Ù…ÙŠÙ‹Ø§ (active/scientific â‰¥ 0.70)
- Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø±ØªØ¨Ø·ÙŠÙ† Ø¹Ù„Ù…ÙŠÙ‹Ø§: Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø±Ù‚Ù… ØªØ±ÙƒÙŠØ² ÙŠØ³Ø§ÙˆÙŠ target_mg "Ø¨Ø§Ù„Ø¸Ø¨Ø·"
- Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ: ØªØ±ØªÙŠØ¨ Ø¹Ø§Ø¯ÙŠ Ø­Ø³Ø¨ final_score (vector + fuzzy: active > scientific > brand)
- Ø«Ù… tie-break Ø¨Ø§Ù„Ù€ base_score10

ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø£Ø³Ù„ÙˆØ¨ drug_search.py:
- normalize_ar
- distance_to_score10
- Ù†ÙØ³ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø®Ø±Ø¬: id/query/base_score10/final_score/bonus/tag/.../meta/doc
"""

from __future__ import annotations
import re
import unicodedata
import difflib
from typing import List, Dict, Any, Optional, Tuple, Set
import logging

import chromadb
from chromadb.config import Settings
from langchain_huggingface import HuggingFaceEmbeddings
from config import Config

# Ù„ÙˆØ¬Ø± Ù…Ø®ØµØµ Ù„Ù„Ù…Ø«Ø§Ø¦Ù„ØŒ ÙŠØªØ¨Ø¹ duaya_api
logger = logging.getLogger("duaya_api.eq")

# ======================= Ø«ÙˆØ§Ø¨Øª ÙˆØ£ÙˆØ²Ø§Ù† =======================
# Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¨ÙˆÙ†ØµØ§Øª: Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ù‘Ø§Ù„Ø© Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø§Ù„Ø¹Ù„Ù…ÙŠØŒ ÙˆØ§Ù„Ø¹Ù„Ù…ÙŠ Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø§Ù„ØªØ¬Ø§Ø±ÙŠ
ACTIVE_FUZZY_MIN        = 0.70
ACTIVE_BASE_BOOST       = 1.4
ACTIVE_STRONG_BOOST     = 1.6   # r>=0.90

SCIENTIFIC_FUZZY_MIN    = 0.70
SCIENTIFIC_BASE_BOOST   = 1.0
SCIENTIFIC_STRONG_BOOST = 1.2   # r>=0.90

BRAND_FUZZY_MIN         = 0.70
BRAND_BASE_BOOST        = 0.7
BRAND_STRONG_BOOST      = 1.0   # r>=0.90 Ø£Ùˆ Ø·ÙˆÙ„ Ù‚Ø±ÙŠØ¨

# Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ
FORM_MATCH_BONUS        = 0.6   # strict=True
FORM_SOFT_BONUS         = 0.2   # strict=False

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† Chroma
DEFAULT_EQ_TOP_K        = 800
DEFAULT_MIN_BASE10      = 0.0

# Ø¹ØªØ¨Ø© Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ù†ØªÙŠØ¬Ø© "Ù…Ø±ØªØ¨Ø·Ø© Ø¹Ù„Ù…ÙŠÙ‹Ø§"
RELEVANT_MIN            = 0.70

# ======================= Ø¨Ø¯Ø§Ø¦Ù„ (Different Actives) =======================
# Ø£ÙˆØ²Ø§Ù† Ø¨Ø¯Ø§Ø¦Ù„ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶/Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ ÙˆØ§Ù„ØªØµÙ†ÙŠÙ/Ø§Ù„Ù‚Ø³Ù…
ALT_DISEASE_OVERLAP_BOOST = 3.0
ALT_CLASS_MATCH_BONUS     = 0.8
ALT_SECTION_MATCH_BONUS   = 0.6

# ======================= ØªØ·Ø¨ÙŠØ¹ ÙˆØ£Ø¯ÙˆØ§Øª Ø¹Ø§Ù…Ø© =======================
_ARABIC_DIACRITICS = re.compile(r"[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06DC\u06DF-\u06ED]")
AR_CHARS = "Ø¡-ÙŠ"
_NON_WORD = re.compile(rf"[^\w{AR_CHARS}]+", re.UNICODE)
_LIST_SEP = re.compile(r"[ØŒ,;Ø›]\s*")

def normalize_ar(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = unicodedata.normalize("NFKC", text)
    text = _ARABIC_DIACRITICS.sub("", text)
    text = (text.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§")
                 .replace("Ù‰","ÙŠ").replace("Ø©","Ù‡")
                 .replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ"))
    return re.sub(r"\s+", " ", text).strip().lower()

def distance_to_score10(d: Optional[float]) -> float:
    """Ø­ÙˆÙ‘Ù„ Ù…Ø³Ø§ÙØ© ÙƒÙˆØ³Ø§ÙŠÙ† (0..2) Ø¥Ù„Ù‰ Ø¯Ø±Ø¬Ø© 0..10 (Ù†ÙØ³ drug_search)."""
    if d is None:
        return 0.0
    s = (1.0 - (d / 2.0)) * 10.0
    return max(0.0, min(10.0, float(s)))

# ======================= Active / Scientific =======================
_SPLIT_ACTIVE = re.compile(r"[,\+\-/\(\)\[\]{}Ø›;ØŒ]|(?:\s+Ùˆ\s+)|(?:\s+and\s+)", re.IGNORECASE)

def extract_active_tokens(meta: Dict[str, Any]) -> List[str]:
    """ÙŠØ³ØªØ®Ø±Ø¬ ØªÙˆÙƒÙ†Ø² Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ù‘Ø§Ù„Ø© + ÙŠØ¹ÙŠØ¯ Ø§Ù„Ø¹Ù„Ù…ÙŠ ÙƒØªÙˆÙƒÙÙ† Ù…Ø³ØªÙ‚Ù„."""
    parts_active = []
    v1 = (meta or {}).get("active_ingredients") or (meta or {}).get("Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ø§Ù„Ø©")
    if isinstance(v1, str) and v1.strip():
        parts_active.append(v1)

    tokens_active: List[str] = []
    if parts_active:
        text = " ØŒ ".join(parts_active)
        raw_tokens = [t.strip() for t in _SPLIT_ACTIVE.split(text) if t and t.strip()]
        for t in raw_tokens:
            tn = normalize_ar(t)
            if len(tn) >= 2:
                tokens_active.append(tn)

    sci = (meta or {}).get("scientific_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ") or ""
    sci_norm = normalize_ar(sci) if isinstance(sci, str) else ""

    return tokens_active + ([sci_norm] if sci_norm else [])

def best_active_fuzzy_ratio(meta: Dict[str, Any], q_norm: str) -> Tuple[float, str]:
    """Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚ ÙØ²ÙŠ Ø¶Ù…Ù† (Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ù‘Ø§Ù„Ø© + Ø§Ù„Ø¹Ù„Ù…ÙŠ ÙƒØªÙˆÙƒÙÙ†)."""
    toks = extract_active_tokens(meta)
    best = 0.0
    best_tok = ""
    for t in toks:
        r = difflib.SequenceMatcher(None, t, q_norm).ratio()
        if r > best:
            best = r
            best_tok = t
    return best, best_tok

def fuzzy_ratio_on_scientific_only(meta: Dict[str, Any], q_norm: str) -> float:
    sci = (meta or {}).get("scientific_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ") or ""
    sci_norm = normalize_ar(sci) if isinstance(sci, str) else ""
    if not sci_norm or not q_norm:
        return 0.0
    return difflib.SequenceMatcher(None, sci_norm, q_norm).ratio()

# ======================= Brand / Commercial =======================
NAME_KEYS = [
    "commercial_name", "Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ", "Ø§Ù„Ø§Ø³Ù…_Ø§Ù„ØªØ¬Ø§Ø±ÙŠ",
    "name", "Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠ",
    "scientific_name", "Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ",
]
def best_brand_field(meta: Dict[str, Any]) -> str:
    for k in NAME_KEYS:
        val = (meta or {}).get(k)
        if isinstance(val, str) and val.strip():
            return val
    return ""

# ======================= Ø£Ù…Ø±Ø§Ø¶/Ø£Ø¹Ø±Ø§Ø¶/ØªØµÙ†ÙŠÙ/Ù‚Ø³Ù… =======================
def _split_list_phrases(text: str) -> List[str]:
    if not isinstance(text, str) or not text.strip():
        return []
    # Ù†ÙØµÙ„ Ø¨Ø§Ù„Ù‚ÙˆØ§Ø·Ø¹ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª
    parts = [p.strip() for p in _LIST_SEP.split(text) if p and p.strip()]
    # ØµÙŠØ§Ù†Ø© Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§
    return [p for p in parts if len(normalize_ar(p)) >= 2]

def extract_diseases_phrases(meta: Dict[str, Any]) -> List[str]:
    v = (meta or {}).get("diseases") or (meta or {}).get("Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ØªÙŠ ÙŠØ¹Ø§Ù„Ø¬Ù‡Ø§") or ""
    return _split_list_phrases(v)

def extract_symptoms_phrases(meta: Dict[str, Any]) -> List[str]:
    v = (meta or {}).get("disease_symptoms") or (meta or {}).get("Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ø±Ø¶") or ""
    return _split_list_phrases(v)

def normalize_set(items: List[str]) -> Set[str]:
    return {normalize_ar(x) for x in items if isinstance(x, str) and x.strip()}

def get_classification(meta: Dict[str, Any]) -> str:
    return (meta or {}).get("classification") or (meta or {}).get("Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ") or ""

def get_section(meta: Dict[str, Any]) -> str:
    return (meta or {}).get("section") or (meta or {}).get("Ø§Ù„Ù‚Ø³Ù…") or ""

def brand_best_token_ratio(brand: str, q_norm: str) -> Tuple[float, str, float]:
    """ÙØ²ÙŠ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ ØªÙˆÙƒÙ† Ù…Ù† Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ + Ø¹Ù‚ÙˆØ¨Ø© Ø·ÙˆÙ„ Ø®ÙÙŠÙØ©."""
    v = normalize_ar(brand)
    if not v or not q_norm:
        return 0.0, "", 1.0
    tokens = [t for t in _NON_WORD.split(v) if t]
    best = (0.0, "", 1.0)
    for t in tokens:
        ratio = difflib.SequenceMatcher(None, t, q_norm).ratio()
        rel_extra = max(0, len(t) - len(q_norm)) / max(len(t), 1)
        penalty = 1.0 - min(0.30, rel_extra)  # 0.7..1.0
        if t.startswith(q_norm) or t.endswith(q_norm):
            ratio = min(1.0, ratio + 0.05)
        if ratio * penalty > best[0] * best[2]:
            best = (ratio, t, penalty)
    return best

# ======================= Ø§Ù„ØªØ±ÙƒÙŠØ² (Ø£Ø±Ù‚Ø§Ù… ÙÙ‚Ø·) =======================
NUM_PATTERN = re.compile(r"\d+(?:\.\d+)?")

def grab_all_numbers(text: str) -> List[float]:
    if not text:
        return []
    return [float(x) for x in NUM_PATTERN.findall(str(text))]

def _concentrations_list_to_text(val: Any) -> str:
    """Ø­ÙˆÙ‘Ù„ Ù‚Ø§Ø¦Ù…Ø© [{Ø§Ø³Ù…: Ø±Ù‚Ù…}] Ø¥Ù„Ù‰ Ù†Øµ Ù…ÙˆØ­Ù‘Ø¯ Ù…Ø«Ù„: 'Ø§Ø³Ù… 20 / Ø§Ø³Ù… 1680'."""
    try:
        if isinstance(val, list):
            parts: List[str] = []
            for item in val:
                if isinstance(item, dict) and len(item) == 1:
                    [(k, v)] = list(item.items())
                    parts.append(f"{k} {v}")
            if parts:
                return " / ".join(parts)
    except Exception:
        pass
    return ""

def collect_concentration_texts(meta: Dict[str, Any]) -> str:
    """
    Ù†Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø£Ø±Ù‚Ø§Ù… ØªØ±ÙƒÙŠØ²:
      - concentrations/Ø§Ù„ØªØ±Ø§ÙƒÙŠØ²
      - name/Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠ
      - active_ingredients/Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ø§Ù„Ø©
      - dosage/Ø§Ù„Ø¬Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹ØªØ§Ø¯Ø©
    """
    parts = []
    for k in ("concentrations", "Ø§Ù„ØªØ±Ø§ÙƒÙŠØ²",
              "name", "Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠ",
              "active_ingredients", "Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ø§Ù„Ø©",
              "dosage", "Ø§Ù„Ø¬Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹ØªØ§Ø¯Ø©"):
        v = (meta or {}).get(k)
        if isinstance(v, str) and v.strip():
            parts.append(v)
        elif k in ("concentrations", "Ø§Ù„ØªØ±Ø§ÙƒÙŠØ²") and isinstance(v, list):
            rendered = _concentrations_list_to_text(v)
            if rendered:
                parts.append(rendered)
    return " | ".join(parts)

def find_number_near_active(text: str, active_norm: str, window: int = 25) -> Optional[float]:
    """
    Ù†Ø­Ø§ÙˆÙ„ Ù†Ù„Ù‚Ø· Ø±Ù‚Ù… Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¯Ø§Ø®Ù„ Ù†Ø§ÙØ°Ø© Ø­Ø±ÙˆÙ.
    Ù…Ø«Ø§Ù„: '... Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„ (500 Ù…Ø¬Ù…) ...' â†’ Ù†Ù„ØªÙ‚Ø· 500.
    """
    if not text or not active_norm:
        return None
    t = normalize_ar(str(text))
    for m in re.finditer(re.escape(active_norm), t):
        start, end = m.span()
        l_ctx = max(0, start - window)
        r_ctx = min(len(t), end + window)
        ctx = t[l_ctx:r_ctx]
        nums = grab_all_numbers(ctx)
        if nums:
            return nums[0]
    return None

def has_exact_concentration(meta: Dict[str, Any], active_norm: str, target_mg: float) -> Tuple[bool, Optional[float]]:
    """
    true Ù„Ùˆ Ù„Ù‚ÙŠÙ†Ø§ Ø±Ù‚Ù… ÙŠØ³Ø§ÙˆÙŠ target_mg:
      - Ø£ÙˆÙ„Ù‹Ø§: Ø±Ù‚Ù… Ù…Ù„Ø§ØµÙ‚ Ù„Ø§Ø³Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (Ù„Ùˆ Ø£Ù…ÙƒÙ†)
      - ÙˆØ¥Ù„Ø§: Ø£ÙŠ Ø±Ù‚Ù… Ø¹Ø§Ù… ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¬Ù…Ù‘Ø¹Ø©
    """
    if target_mg <= 0:
        return False, None
    conc_text = collect_concentration_texts(meta)
    # Ø­Ø§ÙˆÙ„ Ø±Ù‚Ù… Ù…Ù„Ø§ØµÙ‚ Ù„Ø§Ø³Ù… Ø§Ù„Ù…Ø§Ø¯Ø©
    near = find_number_near_active(conc_text, active_norm)
    if near is not None and abs(near - target_mg) < 1e-6:
        return True, near
    # ÙˆØ¥Ù„Ø§: Ø£ÙŠ Ø±Ù‚Ù… Ø¹Ø§Ù… ÙŠØ³Ø§ÙˆÙŠ Ø§Ù„Ù‡Ø¯Ù
    for n in grab_all_numbers(conc_text):
        if abs(n - target_mg) < 1e-6:
            return True, n
    return False, near

# ---- ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ² Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© {Ù…Ø§Ø¯Ø©: Ø±Ù‚Ù…} ÙÙŠ Ø§Ù„Ø®Ø±Ø¬ ----
_SEG_SPLIT = re.compile(r"[\/\+;,Ø›ØŒ]|(?:\s+Ùˆ\s+)|(?:\s+and\s+)", re.IGNORECASE)
_NUM_ONLY = re.compile(r"\d+(?:\.\d+)?")
_UNITS_PATTERN = re.compile(r"\b(?:mg|mcg|ug|g|ml|iu|%|Ù…Ù„Øº|Ù…Ø¬Ù…|Ù…Ù„ØºÙ…|Ù…ÙŠÙ„ÙŠØ¬Ø±Ø§Ù…|Ù…ÙŠÙ„ÙŠØºØ±Ø§Ù…|Ù…ÙŠÙƒØ±ÙˆØ¬Ø±Ø§Ù…|Ù…ÙŠÙƒØ±ÙˆØºØ±Ø§Ù…|Ø¬Ø±Ø§Ù…|Ø¬Ù…|Ù…Ù„)\b", re.IGNORECASE)
_ARABIC_DIGITS = str.maketrans("Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©", "0123456789")

def _to_ascii_digits(text: str) -> str:
    if not isinstance(text, str):
        return str(text)
    return text.translate(_ARABIC_DIGITS)

def _clean_material_name(raw_name: str) -> str:
    if not isinstance(raw_name, str):
        return ""
    s = _to_ascii_digits(raw_name)
    s = _UNITS_PATTERN.sub(" ", s)
    s = re.sub(r"\(.*?\)", " ", s)
    s = re.sub(r"[^A-Za-z\-\s\u0600-\u06FF]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

_NAME_NUM_PATTERN = re.compile(r"([A-Za-z\u0600-\u06FF][A-Za-z\u0600-\u06FF\-\s\(\)]+?)\s*(\d+(?:\.\d+)?)")

def parse_concentrations_from_meta(meta: Dict[str, Any]) -> List[Dict[str, float]]:
    if not isinstance(meta, dict):
        return []
    val = meta.get("concentrations")
    if isinstance(val, list):
        try:
            ok = all(isinstance(x, dict) and len(x) == 1 for x in val)
            return val if ok else []
        except Exception:
            return []
    if not isinstance(val, str) or not val.strip():
        return []
    text = _to_ascii_digits(val)
    text_simple = _UNITS_PATTERN.sub(" ", text)
    pairs: List[Dict[str, float]] = []
    for name, num in _NAME_NUM_PATTERN.findall(text_simple):
        name_clean = _clean_material_name(name)
        if not name_clean:
            continue
        try:
            vf = float(num)
            v = int(vf) if abs(vf - int(vf)) < 1e-9 else vf
            pairs.append({name_clean: v})
        except Exception:
            continue
    if pairs:
        return pairs
    nums = [float(x) for x in _NUM_ONLY.findall(text_simple)]
    if not nums:
        return []
    ai_text = (meta.get("active_ingredients") or meta.get("Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ø§Ù„Ø©") or "")
    if isinstance(ai_text, str) and ai_text.strip():
        raw_tokens = [t.strip() for t in _SEG_SPLIT.split(ai_text) if t and t.strip()]
        names = [_clean_material_name(t) for t in raw_tokens if _clean_material_name(t)]
        if len(names) == len(nums):
            out: List[Dict[str, float]] = []
            for nm, vf in zip(names, nums):
                v = int(vf) if abs(vf - int(vf)) < 1e-9 else vf
                out.append({nm: v})
            return out
    if len(nums) == 1:
        sci = meta.get("scientific_name") or meta.get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ") or ai_text
        nm = _clean_material_name(str(sci or "").strip())
        if nm:
            vf = nums[0]
            v = int(vf) if abs(vf - int(vf)) < 1e-9 else vf
            return [{nm: v}]
    return []

# ======================= Ø£Ø¯ÙˆØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ =======================
def normalize_actives_input(actives: Any) -> List[Tuple[str, float]]:
    """
    ÙŠØ­ÙˆÙ‘Ù„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (Ø¨ØµÙŠØº Ù…Ø±Ù†Ø©) Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© [(name_norm, mg)].
    - ÙŠÙ‚Ø¨Ù„ Ø´ÙƒÙ„ [{"Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„": 500}, {"Ø³ÙˆØ¯ÙˆØ¥ÙŠÙÙŠØ¯Ø±ÙŠÙ†": 30}, ...] Ø£Ùˆ [["Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„", 500], ...]
    - ÙŠØ²ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ù…ÙˆØ­Ù‘Ø¯.
    """
    out: List[Tuple[str, float]] = []
    try:
        if not isinstance(actives, list):
            return out
        for item in actives:
            if isinstance(item, dict) and len(item) == 1:
                [(k, v)] = list(item.items())
                name_norm = normalize_ar(_clean_material_name(str(k)))
                try:
                    mg_val = float(v)
                except Exception:
                    mg_val = 0.0
                if name_norm:
                    out.append((name_norm, mg_val))
            elif isinstance(item, (list, tuple)) and len(item) == 2:
                name_norm = normalize_ar(_clean_material_name(str(item[0])))
                try:
                    mg_val = float(item[1])
                except Exception:
                    mg_val = 0.0
                if name_norm:
                    out.append((name_norm, mg_val))
    except Exception:
        return out
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ù†ÙØ³ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ù…ÙˆØ­Ù‘Ø¯
    seen: Set[str] = set()
    uniq: List[Tuple[str, float]] = []
    for nm, mg in out:
        if nm and nm not in seen:
            seen.add(nm)
            uniq.append((nm, mg))
    return uniq

def brand_best_multi_ratio(brand: str, q_norm_list: List[str]) -> Tuple[float, str, float]:
    """Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚ Ù„Ù„Ù…Ø§Ø±ÙƒØ© Ø¹Ø¨Ø± Ø¹Ø¯Ø© ØªÙˆÙƒÙ†Ø§Øª Ø§Ø³ØªØ¹Ù„Ø§Ù… (Ø£Ø³Ù…Ø§Ø¡ Ù…ÙˆØ§Ø¯ Ù…ØªØ¹Ø¯Ø¯Ø©)."""
    best_ratio = 0.0
    best_tok = ""
    best_pen = 1.0
    for qn in q_norm_list:
        r, t, p = brand_best_token_ratio(brand, qn)
        if r * p > best_ratio * best_pen:
            best_ratio, best_tok, best_pen = r, t, p
    return best_ratio, best_tok, best_pen

# ======================= EquivalentsFinder =======================
class EquivalentsFinder:
    """
    - Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: Active/Scientific Ø£ÙˆÙ„Ù‹Ø§ â†’ Scientific-only â†’ Brand
    - Ø§Ù„ÙØ±Ø² Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¹Ù„Ù…ÙŠÙ‹Ø§ Ø£ÙˆÙ„Ù‹Ø§ØŒ Ø«Ù… exact concentration matchØŒ Ø«Ù… final_score
    """

    def __init__(
        self,
        collection_name: str = Config.COLLECTION_NAME,
        db_dir: str = Config.DB_DIRECTORY,
        top_k: int = DEFAULT_EQ_TOP_K,
        min_base10: float = DEFAULT_MIN_BASE10,
    ) -> None:
        self.collection_name = collection_name
        self.db_dir = db_dir
        self.top_k = top_k
        self.min_base10 = min_base10

        self.client = chromadb.PersistentClient(
            path=db_dir,
            settings=Settings(
                persist_directory=db_dir,
                anonymized_telemetry=False,
                allow_reset=True,
                is_persistent=True,
            )
        )
        self.collection = self.client.get_collection(collection_name)

        self.emb = HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_SETTINGS['model_name'],
            model_kwargs=Config.EMBEDDING_SETTINGS['model_kwargs'],
            encode_kwargs=Config.EMBEDDING_SETTINGS['encode_kwargs'],
        )

    # ---------- Queries Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ ----------
    def _query_active_first(self, q: str) -> Dict[str, Any]:
        q_emb = self.emb.embed_query(f"active ingredients or scientific: {q}")
        return self.collection.query(
            query_embeddings=[q_emb],
            n_results=self.top_k,
            include=["distances", "metadatas", "documents"],
        )

    def _query_scientific_only(self, q: str) -> Dict[str, Any]:
        q_emb = self.emb.embed_query(f"scientific name: {q}")
        return self.collection.query(
            query_embeddings=[q_emb],
            n_results=self.top_k,
            include=["distances", "metadatas", "documents"],
        )

    def _query_brand(self, q: str) -> Dict[str, Any]:
        q_emb = self.emb.embed_query(f"brand or commercial name: {q}")
        return self.collection.query(
            query_embeddings=[q_emb],
            n_results=self.top_k,
            include=["distances", "metadatas", "documents"],
        )

    def _query_by_profile(self, diseases_text: str, symptoms_text: str, classification: str, section: str) -> Dict[str, Any]:
        # Ù†Ø¬Ù…Ø¹ Ø¨Ø±ÙˆÙØ§ÙŠÙ„ Ø¹Ù„Ø§Ø¬ÙŠ Ù…Ø®ØªØµØ± Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        profile_parts: List[str] = []
        if diseases_text:
            profile_parts.append(f"treats diseases: {diseases_text}")
        if symptoms_text:
            profile_parts.append(f"symptoms: {symptoms_text}")
        if classification:
            profile_parts.append(f"classification: {classification}")
        if section:
            profile_parts.append(f"section: {section}")
        profile_query = " | ".join(profile_parts) if profile_parts else ""
        q_emb = self.emb.embed_query(profile_query)
        return self.collection.query(
            query_embeddings=[q_emb],
            n_results=self.top_k,
            include=["distances", "metadatas", "documents"],
        )

    # ---------- Ø§Ù„Ù…Ø¬Ø±Ù‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ----------
    def find_equivalents(
        self,
        active_query: str,
        target_mg: float,
        tolerance_mg: float = 0.0,   # Ù…ÙˆØ¬ÙˆØ¯ Ù„Ù„ØªÙˆØ§ÙÙ‚ ÙÙ‚Ø·
        allow_per_ml: bool = True,   # Ù…ÙˆØ¬ÙˆØ¯ Ù„Ù„ØªÙˆØ§ÙÙ‚ ÙÙ‚Ø·
        target_form: Optional[str] = None,
        strict_form: bool = True,
        limit: int = 50,
        debug: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        - ÙŠØ³ØªØ±Ø¬Ø¹ Ù…Ø±Ø´Ø­ÙŠÙ† Ù…Ù† 3 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ø±ØªÙ‘Ø¨Ø© (active â†’ scientific â†’ brand)
        - ÙŠØ¨Ù†ÙŠ ØªØ¬Ù…ÙŠØ¹Ø© Ù…ÙˆØ­Ù‘Ø¯Ø© Ù„Ù„Ù…Ø±Ø´Ø­ÙŠÙ† (Ø£ÙØ¶Ù„ Ù…Ø³Ø§ÙØ© base Ù„ÙƒÙ„ id)
        - ÙŠØ­Ø³Ø¨ bonus (active/scientific/brand + form)
        - ÙŠÙØ±Ø² Ø£Ø®ÙŠØ±Ù‹Ø§ ÙƒØ§Ù„ØªØ§Ù„ÙŠ:
            (1) Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¹Ù„Ù…ÙŠÙ‹Ø§ Ø£ÙˆÙ„Ù‹Ø§ (active/scientific â‰¥ RELEVANT_MIN)
            (2) Ø¯Ø§Ø®Ù„Ù‡Ù…: exact concentration match = True
            (3) Ø«Ù… final_score Ø£Ø¹Ù„Ù‰
            (4) tie-break: base_score10 Ø£Ø¹Ù„Ù‰
        """
        if not active_query:
            return []

        # 1) Ø§Ø¬Ù…Ø¹ Ù…Ø±Ø´Ø­ÙŠÙ† Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
        q_active   = self._query_active_first(active_query)
        q_science  = self._query_scientific_only(active_query)
        q_brand    = self._query_brand(active_query)

        pools = [q_active, q_science, q_brand]

        # 2) Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†: Ø§Ø®ØªØ± "Ø£ÙØ¶Ù„" (Ø£ØµØºØ± Ù…Ø³Ø§ÙØ© â‡’ Ø£Ø¹Ù„Ù‰ base_score10) Ù„ÙƒÙ„ id
        merged: Dict[str, Dict[str, Any]] = {}  # id -> {dist, meta, doc}
        for res in pools:
            ids   = res.get("ids", [[]])[0]
            dists = res.get("distances", [[]])[0]
            metas = res.get("metadatas", [[]])[0]
            docs  = res.get("documents", [[]])[0]
            for _id, dist, meta, doc in zip(ids, dists, metas, docs):
                if _id not in merged or (dist is not None and dist < merged[_id]["dist"]):
                    merged[_id] = {"dist": dist, "meta": meta, "doc": doc}

        q_norm = normalize_ar(active_query)

        # 3) Ø§Ø¨Ù†Ù Ø§Ù„ØµÙÙˆÙ Ù…Ø¹ Ø§Ù„Ø¨ÙˆÙ†ØµØ§Øª (Ø¨Ø¯ÙˆÙ† Ø¨ÙˆÙ†Øµ ØªØ±ÙƒÙŠØ² â€” Ø³Ù†Ø³ØªØ®Ø¯Ù…Ù‡ Ù„Ù„ÙØ±Ø² ÙÙ‚Ø·)
        rows: List[Dict[str, Any]] = []
        for _id, pack in merged.items():
            dist = pack["dist"]
            meta = pack["meta"]
            doc  = pack["doc"]

            base_score = distance_to_score10(dist)
            if base_score < self.min_base10:
                continue

            tag_parts: List[str] = []
            bonus = 0.0

            # Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ
            is_ok, soft_bonus = form_matches(meta, target_form, strict_form)
            if not is_ok:
                continue
            if target_form and soft_bonus:
                bonus += FORM_MATCH_BONUS if strict_form else FORM_SOFT_BONUS
                tag_parts.append("form_match")

            # Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ù‘Ø§Ù„Ø©
            a_ratio, a_tok = best_active_fuzzy_ratio(meta, q_norm)
            if a_ratio >= ACTIVE_FUZZY_MIN:
                bonus += ACTIVE_BASE_BOOST * a_ratio
                if a_ratio >= 0.90:
                    bonus += ACTIVE_STRONG_BOOST
                    tag_parts.append("active_strong_fuzzy")
                else:
                    tag_parts.append(f"active_fuzzy({a_ratio:.2f})")
            else:
                tag_parts.append("active_miss")

            # Ø§Ù„Ø¹Ù„Ù…ÙŠ ÙÙ‚Ø·
            sci_ratio = fuzzy_ratio_on_scientific_only(meta, q_norm)
            if sci_ratio >= SCIENTIFIC_FUZZY_MIN:
                bonus += SCIENTIFIC_BASE_BOOST * sci_ratio
                if sci_ratio >= 0.90:
                    bonus += SCIENTIFIC_STRONG_BOOST
                    tag_parts.append("sci_strong_fuzzy")
                else:
                    tag_parts.append(f"sci_fuzzy({sci_ratio:.2f})")
            else:
                tag_parts.append("sci_miss")

            # Ø§Ù„ØªØ¬Ø§Ø±ÙŠ (Ø£Ù‚Ù„ ÙˆØ²Ù†)
            brand = best_brand_field(meta)
            b_ratio, b_token, b_pen = brand_best_token_ratio(brand, q_norm)
            if b_ratio >= BRAND_FUZZY_MIN:
                bonus += BRAND_BASE_BOOST * b_ratio * b_pen
                if b_ratio >= 0.90 or (b_token and abs(len(b_token) - len(q_norm)) <= 1):
                    bonus += BRAND_STRONG_BOOST
                    tag_parts.append("brand_strong_fuzzy")
                else:
                    tag_parts.append("brand_fuzzy")
            else:
                tag_parts.append("brand_miss")

            # ÙØ­Øµ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø§Ù„Ø¹Ø§Ù…:
            is_relevant = (a_ratio >= RELEVANT_MIN) or (sci_ratio >= RELEVANT_MIN)

            # ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¯Ø¯ÙŠ Ù„Ù„ØªØ±ÙƒÙŠØ² (Ù„Ù„ÙØ±Ø² ÙÙ‚Ø·ØŒ ÙˆØ¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø±ØªØ¨Ø·ÙŠÙ† Ø¹Ù„Ù…ÙŠÙ‹Ø§ ÙÙ‚Ø·)
            active_norm = q_norm
            conc_exact, conc_num = (False, None)
            if is_relevant:
                conc_exact, conc_num = has_exact_concentration(meta, active_norm, target_mg)

            # Ø·Ø¨Ù‘Ù‚ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ² Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù†Ø¯ ØªÙˆÙØ±Ù‡Ø§
            meta_out = dict(meta or {})
            try:
                parsed_conc = parse_concentrations_from_meta(meta_out)
                if parsed_conc:
                    meta_out["concentrations_raw"] = meta_out.get("concentrations")
                    meta_out["concentrations"] = parsed_conc
            except Exception:
                pass

            row = {
                "id": _id,
                "query": active_query,
                "base_score10": round(base_score, 3),
                "final_score": round(base_score + bonus, 3),
                "bonus": round(bonus, 3),
                "tag": "+".join(tag_parts) if tag_parts else "vector_only",
                "brand": brand or None,
                "name": (meta or {}).get("name") or (meta or {}).get("Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠ"),
                "commercial_name": brand or (meta or {}).get("commercial_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ"),
                "scientific_name": (meta or {}).get("scientific_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ"),
                "manufacturer": (meta or {}).get("manufacturer") or (meta or {}).get("Ø§Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ù…ØµÙ†Ø¹Ø©"),
                "meta": meta_out,
                "doc": doc,
                # Ù…ÙØ§ØªÙŠØ­ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ÙØ±Ø² Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:
                "_relevant": is_relevant,
                "_conc_exact": bool(conc_exact) if is_relevant else False,
                "_conc_num": conc_num,
            }
            rows.append(row)

        # 4) Ø§Ù„ÙØ±Ø² Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:
        #   - Ø§Ù„Ù…Ø±ØªØ¨Ø·ÙˆÙ† Ø¹Ù„Ù…ÙŠÙ‹Ø§ Ø£ÙˆÙ„Ø§Ù‹
        #   - Ø¯Ø§Ø®Ù„Ù‡Ù…: Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ÙˆÙ† ÙÙŠ Ø§Ù„ØªØ±ÙƒÙŠØ²
        #   - Ø«Ù…: final_score Ø£Ø¹Ù„Ù‰
        #   - Ø«Ù…: base_score10 Ø£Ø¹Ù„Ù‰
        rows.sort(key=lambda x: (
            0 if x.get("_relevant") else 1,
            0 if x.get("_conc_exact") else 1,
            -(x["final_score"]),
            -(x["base_score10"])
        ))

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ + limit
        for r in rows:
            r.pop("_relevant", None)
            r.pop("_conc_exact", None)
            r.pop("_conc_num", None)

        if limit:
            rows = rows[:limit]
        return rows

    # ---------- Ø¨Ø¯Ø§Ø¦Ù„ Ø¨Ù…Ø§Ø¯Ø© Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ† Ù†ÙØ³ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª ----------
    def _collect_seed_context(
        self,
        active_query: str,
        target_form: Optional[str],
        strict_form: bool,
        seed_limit: int = 20,
    ) -> Tuple[Set[str], Set[str], str, str]:
        """ÙŠØ¬Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ø¹Ù„Ø§Ø¬ÙŠ (Ø£Ù…Ø±Ø§Ø¶/Ø£Ø¹Ø±Ø§Ø¶/ØªØµÙ†ÙŠÙ/Ù‚Ø³Ù…) Ù…Ù† Ø£ÙØ¶Ù„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø«Ø§Ø¦Ù„ ÙƒØ¨Ø°ÙˆØ±."""
        seeds = self.find_equivalents(
            active_query=active_query,
            target_mg=0.0,
            tolerance_mg=0.0,
            allow_per_ml=True,
            target_form=target_form,
            strict_form=strict_form,
            limit=seed_limit,
            debug=False,
        )
        diseases_union: Set[str] = set()
        symptoms_union: Set[str] = set()
        class_counts: Dict[str, int] = {}
        section_counts: Dict[str, int] = {}
        for r in seeds:
            meta = r.get("meta") or {}
            d = normalize_set(extract_diseases_phrases(meta))
            s = normalize_set(extract_symptoms_phrases(meta))
            diseases_union.update(d)
            symptoms_union.update(s)
            cl = normalize_ar(get_classification(meta))
            sec = normalize_ar(get_section(meta))
            if cl:
                class_counts[cl] = class_counts.get(cl, 0) + 1
            if sec:
                section_counts[sec] = section_counts.get(sec, 0) + 1

        def pick_max(counts: Dict[str, int]) -> str:
            if not counts:
                return ""
            return max(counts.items(), key=lambda kv: kv[1])[0]

        return diseases_union, symptoms_union, pick_max(class_counts), pick_max(section_counts)

    def find_alternatives(
        self,
        active_query: str,
        target_form: Optional[str] = None,
        strict_form: bool = True,
        limit: int = 50,
        exclude_ids: Optional[Set[str]] = None,
        debug: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        ÙŠØ¹Ø«Ø± Ø¹Ù„Ù‰ Ø¨Ø¯Ø§Ø¦Ù„ Ø¨Ù…Ø§Ø¯Ø© ÙØ¹Ø§Ù„Ø© Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ†Ù‡Ø§ ØªØ¹Ø§Ù„Ø¬ Ù†ÙØ³ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶/Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø£Ùˆ ØªÙ‚Ø¹ ØªØ­Øª Ù†ÙØ³ Ø§Ù„ØªØµÙ†ÙŠÙ/Ø§Ù„Ù‚Ø³Ù….
        - ÙŠØ³ØªØ®Ø¯Ù… Ø³ÙŠØ§Ù‚Ù‹Ø§ Ù…Ø³ØªÙ…Ø¯Ù‹Ø§ Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø«Ø§Ø¦Ù„ ÙƒØ¨Ø°Ø±Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù….
        - ÙŠØ³ØªØ¨Ø¹Ø¯ Ø£ÙŠ Ù†ØªÙŠØ¬Ø© ØªØ­ØªÙˆÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ø¯ÙŠÙ„Ù‡Ø§.
        """
        if not active_query:
            return []

        active_norm = normalize_ar(active_query)
        diseases_set, symptoms_set, seed_class, seed_section = self._collect_seed_context(
            active_query, target_form, strict_form, seed_limit=min(20, self.top_k)
        )

        # Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³ÙŠØ§Ù‚ ÙƒØ§ÙÙ
        if not diseases_set and not symptoms_set and not seed_class and not seed_section:
            return []

        diseases_text = "ØŒ ".join(sorted(diseases_set))
        symptoms_text = "ØŒ ".join(sorted(symptoms_set))
        res = self._query_by_profile(diseases_text, symptoms_text, seed_class, seed_section)

        ids   = res.get("ids", [[]])[0]
        dists = res.get("distances", [[]])[0]
        metas = res.get("metadatas", [[]])[0]
        docs  = res.get("documents", [[]])[0]

        seeds_union = diseases_set.union(symptoms_set)

        rows: List[Dict[str, Any]] = []
        seen: Set[str] = set()
        for _id, dist, meta, doc in zip(ids, dists, metas, docs):
            if not _id or _id in seen:
                continue
            seen.add(_id)
            if exclude_ids and _id in exclude_ids:
                continue

            base_score = distance_to_score10(dist)
            if base_score < self.min_base10:
                continue

            # Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ
            is_ok, soft_bonus = form_matches(meta, target_form, strict_form)
            if not is_ok:
                continue

            # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù†ÙØ³ Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø©
            ai = normalize_ar((meta or {}).get("active_ingredients") or (meta or {}).get("Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙØ¹Ø§Ù„Ø©") or "")
            sci = normalize_ar((meta or {}).get("scientific_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ") or "")
            if active_norm and (active_norm in ai or active_norm in sci):
                continue

            brand = best_brand_field(meta)

            cand_d = normalize_set(extract_diseases_phrases(meta))
            cand_s = normalize_set(extract_symptoms_phrases(meta))
            cand_union = cand_d.union(cand_s)

            overlap = 0.0
            if seeds_union:
                inter = seeds_union.intersection(cand_union)
                overlap = len(inter) / max(1, len(seeds_union))

            bonus = 0.0
            tag_parts: List[str] = []
            if soft_bonus and target_form:
                bonus += FORM_MATCH_BONUS if strict_form else FORM_SOFT_BONUS
                tag_parts.append("form_match")

            if overlap > 0:
                bonus += ALT_DISEASE_OVERLAP_BOOST * overlap
                tag_parts.append(f"alt_overlap({overlap:.2f})")
            else:
                tag_parts.append("alt_overlap(0)")

            cand_class = normalize_ar(get_classification(meta))
            cand_section = normalize_ar(get_section(meta))
            if seed_class and cand_class and (seed_class == cand_class or seed_class in cand_class or cand_class in seed_class):
                bonus += ALT_CLASS_MATCH_BONUS
                tag_parts.append("class_match")
            if seed_section and cand_section and (seed_section == cand_section or seed_section in cand_section or cand_section in seed_section):
                bonus += ALT_SECTION_MATCH_BONUS
                tag_parts.append("section_match")

            # Ø·Ø¨Ù‘Ù‚ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ² Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù†Ø¯ ØªÙˆÙØ±Ù‡Ø§
            meta_out = dict(meta or {})
            try:
                parsed_conc = parse_concentrations_from_meta(meta_out)
                if parsed_conc:
                    meta_out["concentrations_raw"] = meta_out.get("concentrations")
                    meta_out["concentrations"] = parsed_conc
            except Exception:
                pass

            row = {
                "id": _id,
                "query": active_query,
                "base_score10": round(base_score, 3),
                "final_score": round(base_score + bonus, 3),
                "bonus": round(bonus, 3),
                "tag": "+".join(tag_parts) if tag_parts else "vector_only",
                "brand": brand or None,
                "name": (meta or {}).get("name") or (meta or {}).get("Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠ"),
                "commercial_name": brand or (meta or {}).get("commercial_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ"),
                "scientific_name": (meta or {}).get("scientific_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ"),
                "manufacturer": (meta or {}).get("manufacturer") or (meta or {}).get("Ø§Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ù…ØµÙ†Ø¹Ø©"),
                "meta": meta_out,
                "doc": doc,
                # Ù…ÙØ§ØªÙŠØ­ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ÙØ±Ø²
                "_overlap": overlap,
                "_base": base_score,
            }
            rows.append(row)

        # Ø§Ù„ÙØ±Ø²: Ø£Ø¹Ù„Ù‰ ØªØ¯Ø§Ø®Ù„ Ø«Ù… final_score Ø«Ù… base
        rows.sort(key=lambda x: (-(x.get("_overlap", 0.0)), -(x["final_score"]), -(x.get("_base", 0.0))))
        for r in rows:
            r.pop("_overlap", None)
            r.pop("_base", None)
        if limit:
            rows = rows[:limit]
        return rows

    def find_equivalents_multi(
        self,
        actives: List[Any],
        target_form: Optional[str] = None,
        strict_form: bool = True,
        limit: int = 50,
        debug: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        Ø¨Ø­Ø« Ù…Ø«Ø§Ø¦Ù„ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙˆØ§Ø¯ ÙØ¹Ù‘Ø§Ù„Ø© Ù…Ø¹ ØªØ±Ø§ÙƒÙŠØ²Ù‡Ø§.
        - ÙŠÙ‚Ø¯Ù‘Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø£ÙˆÙ„Ù‹Ø§ØŒ Ø«Ù… Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø§Ø«Ù†ØªÙŠÙ†ØŒ Ø«Ù… ÙˆØ§Ø­Ø¯Ø©.
        - Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©: ÙŠÙØ¶Ù‘Ù„ Ù…Ù† ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ² Ø§Ù„Ù…Ù‚ØµÙˆØ¯Ø©ØŒ Ø«Ù… final_score.
        actives: Ù‚Ø§Ø¦Ù…Ø© Ù…Ø«Ù„ [{"Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„": 500}, {"Ø³ÙˆØ¯ÙˆØ¥ÙŠÙÙŠØ¯Ø±ÙŠÙ†": 30}, {"Ø¯ÙƒØ³ØªØ±ÙˆÙ…ÙŠØ«ÙˆØ±ÙØ§Ù†": 15}]
        """
        pairs = normalize_actives_input(actives)
        if debug:
            logger.debug("[multi] normalized pairs: %s", pairs)
        if not pairs:
            return []

        # 1) Ø§Ø¬Ù…Ø¹ Ù…Ø±Ø´Ø­ÙŠÙ† Ù…Ù† Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù„ÙƒÙ„ Ù…Ø§Ø¯Ø© + Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ø±ÙƒØ¨Ø© (Ø§Ù„ÙƒÙ„ + Ø£Ø²ÙˆØ§Ø¬)
        q_norm_list = [nm for (nm, _mg) in pairs]
        if debug:
            logger.debug("[multi] q_norm_list: %s", q_norm_list)
        merged: Dict[str, Dict[str, Any]] = {}
        # Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙØ±Ø¯ÙŠØ©
        for qn in q_norm_list:
            for res in (
                self._query_active_first(qn),
                self._query_scientific_only(qn),
                self._query_brand(qn),
            ):
                if debug:
                    ids_dbg = res.get("ids", [[]])[0]
                    logger.debug("[multi] single-query '%s' -> %d hits", qn, len(ids_dbg))
                ids   = res.get("ids", [[]])[0]
                dists = res.get("distances", [[]])[0]
                metas = res.get("metadatas", [[]])[0]
                docs  = res.get("documents", [[]])[0]
                for _id, dist, meta, doc in zip(ids, dists, metas, docs):
                    if _id not in merged or (dist is not None and dist < merged[_id]["dist"]):
                        merged[_id] = {"dist": dist, "meta": meta, "doc": doc}

        # Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø±ÙƒÙ‘Ø¨ Ø¨ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ø¯
        if len(q_norm_list) >= 2:
            joined = "ØŒ ".join(q_norm_list)
            # Ù†ÙˆØ³Ù‘Ø¹ n_results Ù„Ù„ØªØ±ÙƒÙŠØ¨Ø§Øª
            try:
                prompts = [
                    f"active ingredients or scientific: {joined}",
                    f"scientific name: {joined}",
                    f"brand or commercial name: {joined}",
                ]
                for prompt in prompts:
                    q_emb = self.emb.embed_query(prompt)
                    res = self.collection.query(
                        query_embeddings=[q_emb],
                        n_results=min(self.top_k * 3, 5000),
                        include=["distances", "metadatas", "documents"],
                    )
                    if debug:
                        ids_dbg = res.get("ids", [[]])[0]
                        logger.debug("[multi] joined all prompt='%s' -> %d hits", prompt, len(ids_dbg))
                    ids   = res.get("ids", [[]])[0]
                    dists = res.get("distances", [[]])[0]
                    metas = res.get("metadatas", [[]])[0]
                    docs  = res.get("documents", [[]])[0]
                    for _id, dist, meta, doc in zip(ids, dists, metas, docs):
                        if _id not in merged or (dist is not None and dist < merged[_id]["dist"]):
                            merged[_id] = {"dist": dist, "meta": meta, "doc": doc}
            except Exception as e:
                if debug:
                    logger.debug("[multi] joined all query failed: %s", e)

        # Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø£Ø²ÙˆØ§Ø¬ (ØªØ­Ø³ÙŠÙ† ØªØ°ÙƒÙ‘Ø± Ø§Ù„ØªØ±ÙƒÙŠØ¨Ø§Øª)
        if len(q_norm_list) >= 3:
            for i in range(len(q_norm_list)):
                for j in range(i + 1, len(q_norm_list)):
                    pair_join = f"{q_norm_list[i]}ØŒ {q_norm_list[j]}"
                    try:
                        prompts = [
                            f"active ingredients or scientific: {pair_join}",
                            f"scientific name: {pair_join}",
                            f"brand or commercial name: {pair_join}",
                        ]
                        for prompt in prompts:
                            q_emb = self.emb.embed_query(prompt)
                            res = self.collection.query(
                                query_embeddings=[q_emb],
                                n_results=min(self.top_k * 2, 4000),
                                include=["distances", "metadatas", "documents"],
                            )
                            if debug:
                                ids_dbg = res.get("ids", [[]])[0]
                                logger.debug("[multi] pair prompt='%s' -> %d hits", prompt, len(ids_dbg))
                            ids   = res.get("ids", [[]])[0]
                            dists = res.get("distances", [[]])[0]
                            metas = res.get("metadatas", [[]])[0]
                            docs  = res.get("documents", [[]])[0]
                            for _id, dist, meta, doc in zip(ids, dists, metas, docs):
                                if _id not in merged or (dist is not None and dist < merged[_id]["dist"]):
                                    merged[_id] = {"dist": dist, "meta": meta, "doc": doc}
                    except Exception as e:
                        if debug:
                            logger.debug("[multi] pair query failed '%s': %s", pair_join, e)

        if debug:
            logger.debug("[multi] merged candidates: %d", len(merged))

        # 2) Ø§Ø¨Ù†Ù Ø§Ù„ØµÙÙˆÙ Ù…Ø¹ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø§Øª Ø¹Ø¨Ø± ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ø¯
        rows: List[Dict[str, Any]] = []
        for _id, pack in merged.items():
            dist = pack["dist"]
            meta = pack["meta"]
            doc  = pack["doc"]

            base_score = distance_to_score10(dist)
            if base_score < self.min_base10:
                continue

            # Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ
            is_ok, soft_bonus = form_matches(meta, target_form, strict_form)
            if not is_ok:
                continue

            bonus = 0.0
            tag_parts: List[str] = []
            if target_form and soft_bonus:
                bonus += FORM_MATCH_BONUS if strict_form else FORM_SOFT_BONUS
                tag_parts.append("form_match")

            matched_count = 0
            exact_conc_count = 0
            per_active_matches: List[Dict[str, Any]] = []

            for (act_norm, mg) in pairs:
                a_ratio, a_tok = best_active_fuzzy_ratio(meta, act_norm)
                sci_ratio = fuzzy_ratio_on_scientific_only(meta, act_norm)
                relevant = (a_ratio >= RELEVANT_MIN) or (sci_ratio >= RELEVANT_MIN)

                conc_exact = False
                conc_num: Optional[float] = None
                if relevant and mg and mg > 0:
                    conc_exact, conc_num = has_exact_concentration(meta, act_norm, mg)

                if relevant:
                    matched_count += 1
                    if conc_exact:
                        exact_conc_count += 1

                # Ø¨ÙˆÙ†ØµØ§Øª Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©
                if a_ratio >= ACTIVE_FUZZY_MIN:
                    bonus += ACTIVE_BASE_BOOST * a_ratio
                    if a_ratio >= 0.90:
                        bonus += ACTIVE_STRONG_BOOST
                if sci_ratio >= SCIENTIFIC_FUZZY_MIN:
                    bonus += SCIENTIFIC_BASE_BOOST * sci_ratio
                    if sci_ratio >= 0.90:
                        bonus += SCIENTIFIC_STRONG_BOOST

                per_active_matches.append({
                    "active": act_norm,
                    "mg": mg,
                    "active_ratio": round(a_ratio, 3),
                    "scientific_ratio": round(sci_ratio, 3),
                    "relevant": relevant,
                    "conc_exact": bool(conc_exact),
                    "conc_num": conc_num,
                })

            # Ø¨ÙˆÙ†Øµ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚ Ø¹Ø¨Ø± Ø§Ù„Ù…ÙˆØ§Ø¯)
            brand = best_brand_field(meta)
            b_ratio, b_token, b_pen = brand_best_multi_ratio(brand, q_norm_list)
            if b_ratio >= BRAND_FUZZY_MIN:
                bonus += BRAND_BASE_BOOST * b_ratio * b_pen
                if b_ratio >= 0.90 or (b_token and abs(len(b_token) - max(len(q) for q in q_norm_list)) <= 1):
                    bonus += BRAND_STRONG_BOOST
                    tag_parts.append("brand_strong_fuzzy")
                else:
                    tag_parts.append("brand_fuzzy")
            else:
                tag_parts.append("brand_miss")

            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ² Ø¥Ù† Ø£Ù…ÙƒÙ†
            meta_out = dict(meta or {})
            try:
                parsed_conc = parse_concentrations_from_meta(meta_out)
                if parsed_conc:
                    meta_out["concentrations_raw"] = meta_out.get("concentrations")
                    meta_out["concentrations"] = parsed_conc
            except Exception:
                pass

            joined_query = " + ".join([nm for (nm, _m) in pairs])
            row = {
                "id": _id,
                "query": joined_query,
                "base_score10": round(base_score, 3),
                "final_score": round(base_score + bonus, 3),
                "bonus": round(bonus, 3),
                "tag": "+".join(tag_parts) if tag_parts else "vector_only",
                "brand": brand or None,
                "name": (meta or {}).get("name") or (meta or {}).get("Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠ"),
                "commercial_name": brand or (meta or {}).get("commercial_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ"),
                "scientific_name": (meta or {}).get("scientific_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ"),
                "manufacturer": (meta or {}).get("manufacturer") or (meta or {}).get("Ø§Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ù…ØµÙ†Ø¹Ø©"),
                "meta": meta_out,
                "doc": doc,
                # Ù…ÙØ§ØªÙŠØ­ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ÙØ±Ø²
                "_matched_count": matched_count,
                "_exact_conc_count": exact_conc_count,
                "_actives_total": len(pairs),
                # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©
                "matches": per_active_matches,
            }
            # Ø§Ø³ØªØ¨Ø¹Ø¯ Ù…Ù† Ù„ÙŠØ³ Ù„Ø¯ÙŠÙ‡ Ø£ÙŠ ØªØ·Ø§Ø¨Ù‚ Ø°ÙŠ ØµÙ„Ø©
            if matched_count <= 0:
                continue
            rows.append(row)
            if debug and matched_count > 0:
                try:
                    brand_dbg = best_brand_field(meta)
                    logger.debug(
                        "[multi] cand id=%s brand=%s matched=%d exact=%d base=%.3f final=%.3f",
                        _id, brand_dbg, matched_count, exact_conc_count, base_score, row["final_score"],
                    )
                except Exception:
                    pass

        # 3) Ø§Ù„ÙØ±Ø² Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø§Øª Ø«Ù… Ø¹Ø¯Ø¯ ØªØ±Ø§ÙƒÙŠØ² Ù…Ø·Ø§Ø¨Ù‚Ø© Ø«Ù… final_score Ø«Ù… base
        rows.sort(key=lambda x: (
            -x.get("_matched_count", 0),
            -x.get("_exact_conc_count", 0),
            -(x["final_score"]),
            -(x["base_score10"]) 
        ))

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© + limit
        for r in rows:
            r.pop("_matched_count", None)
            r.pop("_exact_conc_count", None)
            r.pop("_actives_total", None)
        if limit:
            rows = rows[:limit]
        return rows

    def _collect_seed_context_multi(
        self,
        actives: List[Any],
        target_form: Optional[str],
        strict_form: bool,
        seed_limit: int = 20,
    ) -> Tuple[Set[str], Set[str], str, str]:
        """ÙŠØ¬Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ø¹Ù„Ø§Ø¬ÙŠ ÙƒØ¨Ø°ÙˆØ± Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ multi-equivalents."""
        seeds = self.find_equivalents_multi(
            actives=actives,
            target_form=target_form,
            strict_form=strict_form,
            limit=seed_limit,
            debug=False,
        )
        diseases_union: Set[str] = set()
        symptoms_union: Set[str] = set()
        class_counts: Dict[str, int] = {}
        section_counts: Dict[str, int] = {}
        for r in seeds:
            meta = r.get("meta") or {}
            d = normalize_set(extract_diseases_phrases(meta))
            s = normalize_set(extract_symptoms_phrases(meta))
            diseases_union.update(d)
            symptoms_union.update(s)
            cl = normalize_ar(get_classification(meta))
            sec = normalize_ar(get_section(meta))
            if cl:
                class_counts[cl] = class_counts.get(cl, 0) + 1
            if sec:
                section_counts[sec] = section_counts.get(sec, 0) + 1

        def pick_max(counts: Dict[str, int]) -> str:
            if not counts:
                return ""
            return max(counts.items(), key=lambda kv: kv[1])[0]

        return diseases_union, symptoms_union, pick_max(class_counts), pick_max(section_counts)

    def find_alternatives_multi(
        self,
        actives: List[Any],
        target_form: Optional[str] = None,
        strict_form: bool = True,
        limit: int = 50,
        exclude_ids: Optional[Set[str]] = None,
        debug: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        Ø¨Ø¯Ø§Ø¦Ù„ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙˆØ§Ø¯ Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ† ØªØ¹Ø§Ù„Ø¬ Ù†ÙØ³ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶/Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶.
        - ÙŠÙØ³ØªØ¨Ø¹Ø¯ Ù…Ù† Ù„Ø¯ÙŠÙ‡ Ø¨Ø§Ù„Ø¶Ø¨Ø· Ù†ÙØ³ ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.
        - Ù„Ùˆ ÙƒØ§Ù† Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ù…Ø±Ø´Ø­ Ø°Ùˆ Ù…Ø§Ø¯Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆØªØ·Ø§Ø¨Ù‚ Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠØ©ØŒ ÙŠÙØ³ØªØ¨Ø¹Ø¯.
        """
        pairs = normalize_actives_input(actives)
        if not pairs:
            return []

        orig_set: Set[str] = {nm for (nm, _mg) in pairs}
        diseases_set, symptoms_set, seed_class, seed_section = self._collect_seed_context_multi(
            actives, target_form, strict_form, seed_limit=min(20, self.top_k)
        )

        if not diseases_set and not symptoms_set and not seed_class and not seed_section:
            return []

        diseases_text = "ØŒ ".join(sorted(diseases_set))
        symptoms_text = "ØŒ ".join(sorted(symptoms_set))
        res = self._query_by_profile(diseases_text, symptoms_text, seed_class, seed_section)

        ids   = res.get("ids", [[]])[0]
        dists = res.get("distances", [[]])[0]
        metas = res.get("metadatas", [[]])[0]
        docs  = res.get("documents", [[]])[0]

        rows: List[Dict[str, Any]] = []
        seen: Set[str] = set()
        for _id, dist, meta, doc in zip(ids, dists, metas, docs):
            if not _id or _id in seen:
                continue
            seen.add(_id)
            if exclude_ids and _id in exclude_ids:
                continue

            base_score = distance_to_score10(dist)
            if base_score < self.min_base10:
                continue

            # Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ
            is_ok, soft_bonus = form_matches(meta, target_form, strict_form)
            if not is_ok:
                continue

            # ØªØ¬Ù…ÙŠØ¹ ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ§Ø¯ Ù„Ù„Ù…Ø±Ø´Ø­
            cand_tokens = set(extract_active_tokens(meta))

            # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù…Ù† ÙŠØ·Ø§Ø¨Ù‚ ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠØ© (Ù†ÙØ³ Ø§Ù„ØªØ±ÙƒÙŠØ¨Ø©)
            if orig_set and orig_set.issubset(cand_tokens):
                continue

            # Ù„Ùˆ Ø§Ù„Ù…Ø±Ø´Ø­ Ø°Ùˆ Ù…Ø§Ø¯Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙ‡ÙŠ Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠØ© â†’ Ø§Ø³ØªØ¨Ø¹Ø¯
            if len(cand_tokens) == 1 and any(t in orig_set for t in cand_tokens):
                continue

            brand = best_brand_field(meta)

            cand_d = normalize_set(extract_diseases_phrases(meta))
            cand_s = normalize_set(extract_symptoms_phrases(meta))
            cand_union = cand_d.union(cand_s)

            seeds_union = diseases_set.union(symptoms_set)
            overlap = 0.0
            if seeds_union:
                inter = seeds_union.intersection(cand_union)
                overlap = len(inter) / max(1, len(seeds_union))

            bonus = 0.0
            tag_parts: List[str] = []
            if soft_bonus and target_form:
                bonus += FORM_MATCH_BONUS if strict_form else FORM_SOFT_BONUS
                tag_parts.append("form_match")
            if overlap > 0:
                bonus += ALT_DISEASE_OVERLAP_BOOST * overlap
                tag_parts.append(f"alt_overlap({overlap:.2f})")
            else:
                tag_parts.append("alt_overlap(0)")

            cand_class = normalize_ar(get_classification(meta))
            cand_section = normalize_ar(get_section(meta))
            if seed_class and cand_class and (seed_class == cand_class or seed_class in cand_class or cand_class in seed_class):
                bonus += ALT_CLASS_MATCH_BONUS
                tag_parts.append("class_match")
            if seed_section and cand_section and (seed_section == cand_section or seed_section in cand_section or cand_section in seed_section):
                bonus += ALT_SECTION_MATCH_BONUS
                tag_parts.append("section_match")

            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ² Ø¥Ù† Ø£Ù…ÙƒÙ†
            meta_out = dict(meta or {})
            try:
                parsed_conc = parse_concentrations_from_meta(meta_out)
                if parsed_conc:
                    meta_out["concentrations_raw"] = meta_out.get("concentrations")
                    meta_out["concentrations"] = parsed_conc
            except Exception:
                pass

            row = {
                "id": _id,
                "query": " + ".join(sorted(list(orig_set))),
                "base_score10": round(base_score, 3),
                "final_score": round(base_score + bonus, 3),
                "bonus": round(bonus, 3),
                "tag": "+".join(tag_parts) if tag_parts else "vector_only",
                "brand": brand or None,
                "name": (meta or {}).get("name") or (meta or {}).get("Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠ"),
                "commercial_name": brand or (meta or {}).get("commercial_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ"),
                "scientific_name": (meta or {}).get("scientific_name") or (meta or {}).get("Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ"),
                "manufacturer": (meta or {}).get("manufacturer") or (meta or {}).get("Ø§Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ù…ØµÙ†Ø¹Ø©"),
                "meta": meta_out,
                "doc": doc,
                # Ù…ÙØ§ØªÙŠØ­ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ÙØ±Ø²
                "_overlap": overlap,
                "_base": base_score,
            }
            rows.append(row)

        rows.sort(key=lambda x: (-(x.get("_overlap", 0.0)), -(x["final_score"]), -(x.get("_base", 0.0))))
        for r in rows:
            r.pop("_overlap", None)
            r.pop("_base", None)
        if limit:
            rows = rows[:limit]
        return rows

# ====== Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ (Ù†ÙØ¹ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù‡Ù†Ø§) ======
def form_matches(meta: Dict[str, Any], target_form: Optional[str], strict_form: bool) -> Tuple[bool, bool]:
    """ÙŠØ±Ø¬Ø¹ (is_ok, soft_bonus_flag)"""
    if not target_form:
        return True, False
    mform = (meta or {}).get("pharma_form") or (meta or {}).get("Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ø¦ÙŠ") or ""
    if not isinstance(mform, str):
        mform = str(mform or "")
    nf = normalize_ar(target_form)
    mf = normalize_ar(mform)
    if not nf:
        return True, False
    if mf == nf or nf in mf or mf in nf:
        return True, True
    return (False, False) if strict_form else (True, True)
